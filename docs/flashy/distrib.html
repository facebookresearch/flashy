<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>flashy.distrib API documentation</title>
<meta name="description" content="Distrib utilities. When running from inside Dora, you can use the `init` function
imported from `dora.distrib`. Also works outside of Dora if â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>flashy.distrib</code></h1>
</header>
<section id="section-intro">
<p>Distrib utilities. When running from inside Dora, you can use the <code>init</code> function
imported from <code>dora.distrib</code>. Also works outside of Dora if distributed has been initialized
manually.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
&#34;&#34;&#34;Distrib utilities. When running from inside Dora, you can use the `init` function
imported from `dora.distrib`. Also works outside of Dora if distributed has been initialized
manually.
&#34;&#34;&#34;
from contextlib import contextmanager
from functools import partial, wraps
import pickle
import typing as tp

import torch
from torch import distributed
from torch.nn.parallel.distributed import DistributedDataParallel
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import DataLoader, Subset

from dora.distrib import rank, world_size, init  # noqa


def rank_zero_only(fn: tp.Callable) -&gt; tp.Callable:
    &#34;&#34;&#34;Function that can be used as a decorator to enable a
    function/method being called only on rank 0.&#34;&#34;&#34;

    @wraps(fn)
    def wrapped_fn(*args: tp.Any, **kwargs: tp.Any) -&gt; tp.Optional[tp.Any]:
        if is_rank_zero():
            return fn(*args, **kwargs)
        return None

    return wrapped_fn


def is_rank_zero():
    return rank() == 0


def is_distributed():
    return world_size() &gt; 1


def all_reduce(tensor: torch.Tensor, op=distributed.ReduceOp.SUM):
    if is_distributed():
        return distributed.all_reduce(tensor, op)


def average_metrics(metrics: tp.Dict[str, float], count=1.):
    &#34;&#34;&#34;Average a dictionary of metrics across all workers, using the optional
    `count` as unormalized weight.
    &#34;&#34;&#34;
    if not is_distributed():
        return metrics
    keys, values = zip(*metrics.items())
    device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
    tensor = torch.tensor(list(values) + [1], device=device, dtype=torch.float32)
    tensor *= count
    all_reduce(tensor)
    averaged = (tensor[:-1] / tensor[-1]).cpu().tolist()
    return dict(zip(keys, averaged))


def wrap(model):
    &#34;&#34;&#34;Wrap a model in DDP if necessary. You can also choose to use `eager_sync_model`
    or `sync_model`.
    &#34;&#34;&#34;
    if is_distributed():
        return DistributedDataParallel(
            model,
            device_ids=[torch.cuda.current_device()],
            output_device=torch.cuda.current_device())
    else:
        return model


def _check_number_of_params(params: tp.List[torch.Tensor]):
    # utility function to check that the number of params in all workers is the same,
    # and thus avoid a deadlock with distributed all reduce.
    if not is_distributed() or not params:
        return
    tensor = torch.tensor([len(params)], device=params[0].device, dtype=torch.long)
    all_reduce(tensor)
    if tensor.item() != len(params) * world_size():
        # If not all the workers have the same number, for at least one of them,
        # this inequality will be verified.
        raise RuntimeError(f&#34;Mismatch in number of params: ours is {len(params)}, &#34;
                           &#34;at least one worker has a different one.&#34;)


def _is_complex_or_float(tensor):
    return torch.is_floating_point(tensor) or torch.is_complex(tensor)


def average_tensors(tensors: tp.Iterable[torch.Tensor]):
    &#34;&#34;&#34;All reduce averaging of the given tensor list.
    Note that non complex/floating point values are ignored.
    &#34;&#34;&#34;
    if not is_distributed():
        return
    tensors = [tensor for tensor in tensors if _is_complex_or_float(tensor)]
    _check_number_of_params(tensors)
    handles = []
    for tensor in tensors:
        handle = torch.distributed.all_reduce(
            tensor.data, op=torch.distributed.ReduceOp.SUM, async_op=True)
        handles.append((tensor, handle))
    for tensor, handle in handles:
        handle.wait()
        tensor.data /= world_size()


def broadcast_tensors(tensors: tp.Iterable[torch.Tensor], src: int = 0):
    &#34;&#34;&#34;Broadcast the tensors from the given parameters to all workers.
    This can be used to ensure that all workers have the same model to start with.
    &#34;&#34;&#34;
    if not is_distributed():
        return
    tensors = [tensor for tensor in tensors if _is_complex_or_float(tensor)]
    _check_number_of_params(tensors)
    handles = []
    for tensor in tensors:
        handle = distributed.broadcast(tensor.data, src=src, async_op=True)
        handles.append(handle)
    for handle in handles:
        handle.wait()


def broadcast_model(model: torch.nn.Module, src: int = 0):
    &#34;&#34;&#34;Broadcast params and buffers from the given model to all workers.&#34;&#34;&#34;
    broadcast_tensors(model.parameters(), src)
    broadcast_tensors(model.buffers(), src)


def sync_gradients(params: tp.Iterable[torch.Tensor]):
    &#34;&#34;&#34;
    Average gradients from the given parameter list.
    This allows a simpler alternative to DistributedDataParallel.
    For a more complete alternative use `sync_model`, which also synchronizes
    buffers.

    See `eager_sync_gradients` for starting all reduce as soon as gradients
    are available during the backward.

    ..Warning:: This will only synchronize gradients, for full model synchronization
        including buffers, use `sync_model`.
    &#34;&#34;&#34;
    grads = [param.grad for param in params if param.grad is not None]
    average_tensors(grads)


@contextmanager
def eager_sync_gradients(params: tp.Iterable[torch.Tensor]):
    &#34;&#34;&#34;Similar to `sync_gradients`, except this is a context manager that will start syncing
    gradient as soon as they become available. This can be faster, but requires backward to be
    called no more than once!

    ..Warning:: This will only synchronize gradients, for full model synchronization
        including buffers, use `eager_sync_model`.
    &#34;&#34;&#34;
    if not is_distributed():
        yield
        return
    params = list([p for p in params if p.requires_grad])
    _check_number_of_params(params)
    hooks = []
    handles = []
    waiting_params = set(params)

    def _callback(param, grad):
        if param not in waiting_params:
            raise RuntimeError(f&#34;We got a gradient twice for parameter {param}.&#34;)
        handle = torch.distributed.all_reduce(grad.data, op=torch.distributed.ReduceOp.SUM, async_op=True)
        handles.append((param, grad.data, handle))
        waiting_params.remove(param)

    for param in params:
        hooks.append(param.register_hook(partial(_callback, param)))

    try:
        yield
    finally:
        for hook in hooks:
            hook.remove()
        _check_number_of_params(list(waiting_params))  # verify all workers have the same nb of remaining params.
        for param, grad, handle in handles:
            handle.wait()
            assert param.grad is not None
            torch.div(grad, world_size(), out=param.grad)


def sync_model(model: torch.nn.Module, sync_buffers: bool = True, average_buffers: bool = True):
    &#34;&#34;&#34;
    Simpler alternative to DistributedDataParallel, that doesn&#39;t rely
    on any black magic. For simple models it can also be as fast.
    Just call this on your model after the backward is completed.

    Args:
        model: model to synchronize.
        sync_buffers: if True (the default), also synchronizes buffers.
        average_buffers: if True (the default), average buffers, instead
            broadcast from worker 0 (like DDP).
    &#34;&#34;&#34;
    sync_gradients(model.parameters())
    if sync_buffers:
        if average_buffers:
            average_tensors(model.buffers())
        else:
            broadcast_tensors(model.buffers())


@contextmanager
def eager_sync_model(model: torch.nn.Module, sync_buffers: bool = True,
                     average_buffers: bool = True):
    &#34;&#34;&#34;Same as `sync_model` but using `eager_sync_gradients`.
    &#34;&#34;&#34;
    with eager_sync_gradients(model.parameters()):
        yield
    if sync_buffers:
        if average_buffers:
            average_tensors(model.buffers())
        else:
            broadcast_tensors(model.buffers())


def loader(dataset, *args, shuffle=False, klass=DataLoader, **kwargs):
    &#34;&#34;&#34;
    Create a dataloader properly in case of distributed training.
    If a gradient is going to be computed you must set `shuffle=True`.
    &#34;&#34;&#34;
    if not is_distributed():
        return klass(dataset, *args, shuffle=shuffle, **kwargs)

    if shuffle:
        # train means we will compute backward, we use DistributedSampler
        sampler = DistributedSampler(dataset)
        # We ignore shuffle, DistributedSampler already shuffles
        return klass(dataset, *args, **kwargs, sampler=sampler)
    else:
        # We make a manual shard, as DistributedSampler otherwise replicate some examples
        dataset = Subset(dataset, list(range(rank(), len(dataset), world_size())))
        return klass(dataset, *args, shuffle=shuffle, **kwargs)


def broadcast_object(obj: tp.Any = None, src: int = 0, device=None):
    &#34;&#34;&#34;Share the given object (must be picklable) from the worker with rank `src`.
    &#34;&#34;&#34;
    if not is_distributed():
        return obj
    if device is None:
        device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
    size = torch.empty(1, device=device, dtype=torch.long)
    if rank() == src:
        dump = bytearray(pickle.dumps(obj))
        # bytearray made a writable copy to avoid PyTorch warning later on.
        size[0] = len(dump)
    torch.distributed.broadcast(size, src=src)
    # size variable is now set to the length of pickled obj in all processes

    if rank() == src:
        buffer = torch.frombuffer(dump, dtype=torch.uint8).to(device=device)
    else:
        buffer = torch.empty(int(size[0].item()), device=device, dtype=torch.uint8)
    torch.distributed.broadcast(buffer, src=src)
    # buffer variable is now set to pickled obj in all processes
    if rank != src:
        obj = pickle.loads(buffer.cpu().numpy().tobytes())
    return obj


def barrier() -&gt; None:
    &#34;&#34;&#34;Barrier for all workers, when distributed is used.
    &#34;&#34;&#34;
    if is_distributed():
        torch.distributed.barrier()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="flashy.distrib.all_reduce"><code class="name flex">
<span>def <span class="ident">all_reduce</span></span>(<span>tensor:Â torch.Tensor, op=&lt;RedOpType.SUM: 0&gt;)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_reduce(tensor: torch.Tensor, op=distributed.ReduceOp.SUM):
    if is_distributed():
        return distributed.all_reduce(tensor, op)</code></pre>
</details>
</dd>
<dt id="flashy.distrib.average_metrics"><code class="name flex">
<span>def <span class="ident">average_metrics</span></span>(<span>metrics:Â Dict[str,Â float], count=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Average a dictionary of metrics across all workers, using the optional
<code>count</code> as unormalized weight.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def average_metrics(metrics: tp.Dict[str, float], count=1.):
    &#34;&#34;&#34;Average a dictionary of metrics across all workers, using the optional
    `count` as unormalized weight.
    &#34;&#34;&#34;
    if not is_distributed():
        return metrics
    keys, values = zip(*metrics.items())
    device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
    tensor = torch.tensor(list(values) + [1], device=device, dtype=torch.float32)
    tensor *= count
    all_reduce(tensor)
    averaged = (tensor[:-1] / tensor[-1]).cpu().tolist()
    return dict(zip(keys, averaged))</code></pre>
</details>
</dd>
<dt id="flashy.distrib.average_tensors"><code class="name flex">
<span>def <span class="ident">average_tensors</span></span>(<span>tensors:Â Iterable[torch.Tensor])</span>
</code></dt>
<dd>
<div class="desc"><p>All reduce averaging of the given tensor list.
Note that non complex/floating point values are ignored.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def average_tensors(tensors: tp.Iterable[torch.Tensor]):
    &#34;&#34;&#34;All reduce averaging of the given tensor list.
    Note that non complex/floating point values are ignored.
    &#34;&#34;&#34;
    if not is_distributed():
        return
    tensors = [tensor for tensor in tensors if _is_complex_or_float(tensor)]
    _check_number_of_params(tensors)
    handles = []
    for tensor in tensors:
        handle = torch.distributed.all_reduce(
            tensor.data, op=torch.distributed.ReduceOp.SUM, async_op=True)
        handles.append((tensor, handle))
    for tensor, handle in handles:
        handle.wait()
        tensor.data /= world_size()</code></pre>
</details>
</dd>
<dt id="flashy.distrib.barrier"><code class="name flex">
<span>def <span class="ident">barrier</span></span>(<span>) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Barrier for all workers, when distributed is used.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def barrier() -&gt; None:
    &#34;&#34;&#34;Barrier for all workers, when distributed is used.
    &#34;&#34;&#34;
    if is_distributed():
        torch.distributed.barrier()</code></pre>
</details>
</dd>
<dt id="flashy.distrib.broadcast_model"><code class="name flex">
<span>def <span class="ident">broadcast_model</span></span>(<span>model:Â torch.nn.modules.module.Module, src:Â intÂ =Â 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Broadcast params and buffers from the given model to all workers.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def broadcast_model(model: torch.nn.Module, src: int = 0):
    &#34;&#34;&#34;Broadcast params and buffers from the given model to all workers.&#34;&#34;&#34;
    broadcast_tensors(model.parameters(), src)
    broadcast_tensors(model.buffers(), src)</code></pre>
</details>
</dd>
<dt id="flashy.distrib.broadcast_object"><code class="name flex">
<span>def <span class="ident">broadcast_object</span></span>(<span>obj:Â AnyÂ =Â None, src:Â intÂ =Â 0, device=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Share the given object (must be picklable) from the worker with rank <code>src</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def broadcast_object(obj: tp.Any = None, src: int = 0, device=None):
    &#34;&#34;&#34;Share the given object (must be picklable) from the worker with rank `src`.
    &#34;&#34;&#34;
    if not is_distributed():
        return obj
    if device is None:
        device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
    size = torch.empty(1, device=device, dtype=torch.long)
    if rank() == src:
        dump = bytearray(pickle.dumps(obj))
        # bytearray made a writable copy to avoid PyTorch warning later on.
        size[0] = len(dump)
    torch.distributed.broadcast(size, src=src)
    # size variable is now set to the length of pickled obj in all processes

    if rank() == src:
        buffer = torch.frombuffer(dump, dtype=torch.uint8).to(device=device)
    else:
        buffer = torch.empty(int(size[0].item()), device=device, dtype=torch.uint8)
    torch.distributed.broadcast(buffer, src=src)
    # buffer variable is now set to pickled obj in all processes
    if rank != src:
        obj = pickle.loads(buffer.cpu().numpy().tobytes())
    return obj</code></pre>
</details>
</dd>
<dt id="flashy.distrib.broadcast_tensors"><code class="name flex">
<span>def <span class="ident">broadcast_tensors</span></span>(<span>tensors:Â Iterable[torch.Tensor], src:Â intÂ =Â 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Broadcast the tensors from the given parameters to all workers.
This can be used to ensure that all workers have the same model to start with.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def broadcast_tensors(tensors: tp.Iterable[torch.Tensor], src: int = 0):
    &#34;&#34;&#34;Broadcast the tensors from the given parameters to all workers.
    This can be used to ensure that all workers have the same model to start with.
    &#34;&#34;&#34;
    if not is_distributed():
        return
    tensors = [tensor for tensor in tensors if _is_complex_or_float(tensor)]
    _check_number_of_params(tensors)
    handles = []
    for tensor in tensors:
        handle = distributed.broadcast(tensor.data, src=src, async_op=True)
        handles.append(handle)
    for handle in handles:
        handle.wait()</code></pre>
</details>
</dd>
<dt id="flashy.distrib.eager_sync_gradients"><code class="name flex">
<span>def <span class="ident">eager_sync_gradients</span></span>(<span>params:Â Iterable[torch.Tensor])</span>
</code></dt>
<dd>
<div class="desc"><p>Similar to <code><a title="flashy.distrib.sync_gradients" href="#flashy.distrib.sync_gradients">sync_gradients()</a></code>, except this is a context manager that will start syncing
gradient as soon as they become available. This can be faster, but requires backward to be
called no more than once!</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;This will only synchronize gradients, for full model synchronization</p>
<p>including buffers, use <code><a title="flashy.distrib.eager_sync_model" href="#flashy.distrib.eager_sync_model">eager_sync_model()</a></code>.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def eager_sync_gradients(params: tp.Iterable[torch.Tensor]):
    &#34;&#34;&#34;Similar to `sync_gradients`, except this is a context manager that will start syncing
    gradient as soon as they become available. This can be faster, but requires backward to be
    called no more than once!

    ..Warning:: This will only synchronize gradients, for full model synchronization
        including buffers, use `eager_sync_model`.
    &#34;&#34;&#34;
    if not is_distributed():
        yield
        return
    params = list([p for p in params if p.requires_grad])
    _check_number_of_params(params)
    hooks = []
    handles = []
    waiting_params = set(params)

    def _callback(param, grad):
        if param not in waiting_params:
            raise RuntimeError(f&#34;We got a gradient twice for parameter {param}.&#34;)
        handle = torch.distributed.all_reduce(grad.data, op=torch.distributed.ReduceOp.SUM, async_op=True)
        handles.append((param, grad.data, handle))
        waiting_params.remove(param)

    for param in params:
        hooks.append(param.register_hook(partial(_callback, param)))

    try:
        yield
    finally:
        for hook in hooks:
            hook.remove()
        _check_number_of_params(list(waiting_params))  # verify all workers have the same nb of remaining params.
        for param, grad, handle in handles:
            handle.wait()
            assert param.grad is not None
            torch.div(grad, world_size(), out=param.grad)</code></pre>
</details>
</dd>
<dt id="flashy.distrib.eager_sync_model"><code class="name flex">
<span>def <span class="ident">eager_sync_model</span></span>(<span>model:Â torch.nn.modules.module.Module, sync_buffers:Â boolÂ =Â True, average_buffers:Â boolÂ =Â True)</span>
</code></dt>
<dd>
<div class="desc"><p>Same as <code><a title="flashy.distrib.sync_model" href="#flashy.distrib.sync_model">sync_model()</a></code> but using <code><a title="flashy.distrib.eager_sync_gradients" href="#flashy.distrib.eager_sync_gradients">eager_sync_gradients()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def eager_sync_model(model: torch.nn.Module, sync_buffers: bool = True,
                     average_buffers: bool = True):
    &#34;&#34;&#34;Same as `sync_model` but using `eager_sync_gradients`.
    &#34;&#34;&#34;
    with eager_sync_gradients(model.parameters()):
        yield
    if sync_buffers:
        if average_buffers:
            average_tensors(model.buffers())
        else:
            broadcast_tensors(model.buffers())</code></pre>
</details>
</dd>
<dt id="flashy.distrib.is_distributed"><code class="name flex">
<span>def <span class="ident">is_distributed</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_distributed():
    return world_size() &gt; 1</code></pre>
</details>
</dd>
<dt id="flashy.distrib.is_rank_zero"><code class="name flex">
<span>def <span class="ident">is_rank_zero</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_rank_zero():
    return rank() == 0</code></pre>
</details>
</dd>
<dt id="flashy.distrib.loader"><code class="name flex">
<span>def <span class="ident">loader</span></span>(<span>dataset, *args, shuffle=False, klass=torch.utils.data.dataloader.DataLoader, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a dataloader properly in case of distributed training.
If a gradient is going to be computed you must set <code>shuffle=True</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def loader(dataset, *args, shuffle=False, klass=DataLoader, **kwargs):
    &#34;&#34;&#34;
    Create a dataloader properly in case of distributed training.
    If a gradient is going to be computed you must set `shuffle=True`.
    &#34;&#34;&#34;
    if not is_distributed():
        return klass(dataset, *args, shuffle=shuffle, **kwargs)

    if shuffle:
        # train means we will compute backward, we use DistributedSampler
        sampler = DistributedSampler(dataset)
        # We ignore shuffle, DistributedSampler already shuffles
        return klass(dataset, *args, **kwargs, sampler=sampler)
    else:
        # We make a manual shard, as DistributedSampler otherwise replicate some examples
        dataset = Subset(dataset, list(range(rank(), len(dataset), world_size())))
        return klass(dataset, *args, shuffle=shuffle, **kwargs)</code></pre>
</details>
</dd>
<dt id="flashy.distrib.rank_zero_only"><code class="name flex">
<span>def <span class="ident">rank_zero_only</span></span>(<span>fn:Â Callable) â€‘>Â Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Function that can be used as a decorator to enable a
function/method being called only on rank 0.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rank_zero_only(fn: tp.Callable) -&gt; tp.Callable:
    &#34;&#34;&#34;Function that can be used as a decorator to enable a
    function/method being called only on rank 0.&#34;&#34;&#34;

    @wraps(fn)
    def wrapped_fn(*args: tp.Any, **kwargs: tp.Any) -&gt; tp.Optional[tp.Any]:
        if is_rank_zero():
            return fn(*args, **kwargs)
        return None

    return wrapped_fn</code></pre>
</details>
</dd>
<dt id="flashy.distrib.sync_gradients"><code class="name flex">
<span>def <span class="ident">sync_gradients</span></span>(<span>params:Â Iterable[torch.Tensor])</span>
</code></dt>
<dd>
<div class="desc"><p>Average gradients from the given parameter list.
This allows a simpler alternative to DistributedDataParallel.
For a more complete alternative use <code><a title="flashy.distrib.sync_model" href="#flashy.distrib.sync_model">sync_model()</a></code>, which also synchronizes
buffers.</p>
<p>See <code><a title="flashy.distrib.eager_sync_gradients" href="#flashy.distrib.eager_sync_gradients">eager_sync_gradients()</a></code> for starting all reduce as soon as gradients
are available during the backward.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;This will only synchronize gradients, for full model synchronization</p>
<p>including buffers, use <code><a title="flashy.distrib.sync_model" href="#flashy.distrib.sync_model">sync_model()</a></code>.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sync_gradients(params: tp.Iterable[torch.Tensor]):
    &#34;&#34;&#34;
    Average gradients from the given parameter list.
    This allows a simpler alternative to DistributedDataParallel.
    For a more complete alternative use `sync_model`, which also synchronizes
    buffers.

    See `eager_sync_gradients` for starting all reduce as soon as gradients
    are available during the backward.

    ..Warning:: This will only synchronize gradients, for full model synchronization
        including buffers, use `sync_model`.
    &#34;&#34;&#34;
    grads = [param.grad for param in params if param.grad is not None]
    average_tensors(grads)</code></pre>
</details>
</dd>
<dt id="flashy.distrib.sync_model"><code class="name flex">
<span>def <span class="ident">sync_model</span></span>(<span>model:Â torch.nn.modules.module.Module, sync_buffers:Â boolÂ =Â True, average_buffers:Â boolÂ =Â True)</span>
</code></dt>
<dd>
<div class="desc"><p>Simpler alternative to DistributedDataParallel, that doesn't rely
on any black magic. For simple models it can also be as fast.
Just call this on your model after the backward is completed.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>model to synchronize.</dd>
<dt><strong><code>sync_buffers</code></strong></dt>
<dd>if True (the default), also synchronizes buffers.</dd>
<dt><strong><code>average_buffers</code></strong></dt>
<dd>if True (the default), average buffers, instead
broadcast from worker 0 (like DDP).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sync_model(model: torch.nn.Module, sync_buffers: bool = True, average_buffers: bool = True):
    &#34;&#34;&#34;
    Simpler alternative to DistributedDataParallel, that doesn&#39;t rely
    on any black magic. For simple models it can also be as fast.
    Just call this on your model after the backward is completed.

    Args:
        model: model to synchronize.
        sync_buffers: if True (the default), also synchronizes buffers.
        average_buffers: if True (the default), average buffers, instead
            broadcast from worker 0 (like DDP).
    &#34;&#34;&#34;
    sync_gradients(model.parameters())
    if sync_buffers:
        if average_buffers:
            average_tensors(model.buffers())
        else:
            broadcast_tensors(model.buffers())</code></pre>
</details>
</dd>
<dt id="flashy.distrib.wrap"><code class="name flex">
<span>def <span class="ident">wrap</span></span>(<span>model)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrap a model in DDP if necessary. You can also choose to use <code><a title="flashy.distrib.eager_sync_model" href="#flashy.distrib.eager_sync_model">eager_sync_model()</a></code>
or <code><a title="flashy.distrib.sync_model" href="#flashy.distrib.sync_model">sync_model()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wrap(model):
    &#34;&#34;&#34;Wrap a model in DDP if necessary. You can also choose to use `eager_sync_model`
    or `sync_model`.
    &#34;&#34;&#34;
    if is_distributed():
        return DistributedDataParallel(
            model,
            device_ids=[torch.cuda.current_device()],
            output_device=torch.cuda.current_device())
    else:
        return model</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="flashy" href="index.html">flashy</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="flashy.distrib.all_reduce" href="#flashy.distrib.all_reduce">all_reduce</a></code></li>
<li><code><a title="flashy.distrib.average_metrics" href="#flashy.distrib.average_metrics">average_metrics</a></code></li>
<li><code><a title="flashy.distrib.average_tensors" href="#flashy.distrib.average_tensors">average_tensors</a></code></li>
<li><code><a title="flashy.distrib.barrier" href="#flashy.distrib.barrier">barrier</a></code></li>
<li><code><a title="flashy.distrib.broadcast_model" href="#flashy.distrib.broadcast_model">broadcast_model</a></code></li>
<li><code><a title="flashy.distrib.broadcast_object" href="#flashy.distrib.broadcast_object">broadcast_object</a></code></li>
<li><code><a title="flashy.distrib.broadcast_tensors" href="#flashy.distrib.broadcast_tensors">broadcast_tensors</a></code></li>
<li><code><a title="flashy.distrib.eager_sync_gradients" href="#flashy.distrib.eager_sync_gradients">eager_sync_gradients</a></code></li>
<li><code><a title="flashy.distrib.eager_sync_model" href="#flashy.distrib.eager_sync_model">eager_sync_model</a></code></li>
<li><code><a title="flashy.distrib.is_distributed" href="#flashy.distrib.is_distributed">is_distributed</a></code></li>
<li><code><a title="flashy.distrib.is_rank_zero" href="#flashy.distrib.is_rank_zero">is_rank_zero</a></code></li>
<li><code><a title="flashy.distrib.loader" href="#flashy.distrib.loader">loader</a></code></li>
<li><code><a title="flashy.distrib.rank_zero_only" href="#flashy.distrib.rank_zero_only">rank_zero_only</a></code></li>
<li><code><a title="flashy.distrib.sync_gradients" href="#flashy.distrib.sync_gradients">sync_gradients</a></code></li>
<li><code><a title="flashy.distrib.sync_model" href="#flashy.distrib.sync_model">sync_model</a></code></li>
<li><code><a title="flashy.distrib.wrap" href="#flashy.distrib.wrap">wrap</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>